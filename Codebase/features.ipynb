{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import sys\n",
    "import networkx as nx\n",
    "import spacy\n",
    "import traceback\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "random.seed(100)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff79a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNetwork(sentence_list, ent1_word_idx, ent2_word_idx):\n",
    "    try:\n",
    "        sentence = ' '.join(sentence_list)\n",
    "        doc = nlp(sentence)\n",
    "        edges = []\n",
    "        for word_idx, token in enumerate(doc):\n",
    "            for child in token.children:\n",
    "                edges.append((token.i,\n",
    "                          child.i))\n",
    "\n",
    "        graph = nx.Graph(edges)\n",
    "        ## If shortest path not found\n",
    "        try:\n",
    "            shortest_path_length = nx.shortest_path_length(graph, source=ent1_word_idx, target=ent2_word_idx)\n",
    "            shortest_path = nx.shortest_path(graph, source=ent1_word_idx, target=ent2_word_idx)\n",
    "        except: \n",
    "            shortest_path_length = -1\n",
    "            shortest_path = \"no_path_found\"\n",
    "        return shortest_path_length, shortest_path\n",
    "    except:\n",
    "        print(\"NETWORK - NO PATH FOUND: \", sentence_list, ent1_word_idx, ent2_word_idx)\n",
    "        print(traceback.format_exc())\n",
    "        return -1, \"no_path_found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae270f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndex(row, sentence, entity):\n",
    "    try:\n",
    "        return sentence.index(entity.split(\" \")[0]) + 1\n",
    "    except:\n",
    "        print(\"INDEX NOT FOUND - \", sentence, entity, entity.split(\" \")[0])\n",
    "        print(traceback.format_exc())\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(file):\n",
    "    try:\n",
    "        df = pd.read_csv(file, sep=\"\\t\", index_col=False, dtype={'text': str, 'entity type': str, 'pos':str , 'tag': str, 'dep': str, 'sent_index': int, 'entity mention ID': str})                           \n",
    "        df = df.fillna(\"\")\n",
    "        filtered_df = df.where(df[\"entity mention ID\"]!=\"\").groupby([\"sent_index\", \"entity mention ID\"], as_index=False).agg({'text': ' '.join, 'entity type': '-'.join, 'pos' : '-'.join, 'tag' : '-'.join,'dep': '-'.join})\n",
    "        ## Calculating entity-entity distances per sentence\n",
    "        sent_df = filtered_df.groupby([\"sent_index\"], as_index=False).agg({'text': list, 'entity type': list, 'pos' : list, 'tag' : list, 'dep': list})\n",
    "        text_pairs = []\n",
    "        ent_type_pairs = []\n",
    "        pos_pairs = []\n",
    "        tag_pairs = []\n",
    "        dep_pairs = []\n",
    "\n",
    "        for index, row in sent_df.iterrows():\n",
    "            text_pairs.append(list(combinations(row[\"text\"], r=2)))\n",
    "            ent_type_pairs.append(list(combinations(row[\"entity type\"], r=2)))\n",
    "            pos_pairs.append(list(combinations(row[\"pos\"], r=2)))\n",
    "            dep_pairs.append(list(combinations(row[\"dep\"], r=2)))\n",
    "            tag_pairs.append(list(combinations(row[\"tag\"], r=2)))\n",
    "\n",
    "        sent_entity_df = pd.DataFrame()\n",
    "        sent_entity_df[\"sent_index\"] = sent_df[\"sent_index\"]\n",
    "        sent_entity_df[\"entity_pairs\"] = text_pairs\n",
    "        sent_entity_df[\"ent_type_pairs\"] = ent_type_pairs\n",
    "        sent_entity_df[\"pos_pairs\"] = pos_pairs\n",
    "        sent_entity_df[\"dep_pairs\"] = dep_pairs\n",
    "        sent_entity_df[\"tag_pairs\"] = tag_pairs\n",
    "\n",
    "\n",
    "        final_df = sent_entity_df.set_index('sent_index').apply(lambda x: x.apply(pd.Series).stack()).reset_index().drop('level_1', 1)\n",
    "\n",
    "        sentences = df.groupby(['sent_index'], as_index=False).agg({'text': list, 'dep': list})\n",
    "        sentences[\"sentence_length\"] = sentences.apply(lambda x : len(x[\"text\"]), axis=1)\n",
    "        sentences[\"root_index\"] = sentences.apply(lambda x : int(x[\"dep\"].index(\"ROOT\")), axis=1)\n",
    "        sentences[\"root_word\"] = sentences.apply(lambda x : x[\"text\"][x[\"root_index\"]], axis=1)\n",
    "        sentences = sentences.rename(columns={\"text\": \"sentence\"})\n",
    "        final_df = pd.merge(final_df, sentences, on='sent_index', sort=False)\n",
    "        final_df[\"entity_1_index\"] = final_df.apply(lambda x : getIndex(x, x['sentence'], x['entity_pairs'][0]), axis=1)\n",
    "        final_df[\"entity_2_index\"] = final_df.apply(lambda x : getIndex(x, x['sentence'], x['entity_pairs'][1]), axis=1)\n",
    "        final_df[\"entity_distance\"] = final_df.apply(lambda x : x['entity_2_index'] - x['entity_1_index'], axis=1)\n",
    "        final_df[\"no_words_before_entity_1\"] = final_df[\"entity_1_index\"] - 1 \n",
    "        final_df[\"no_words_after_entity_2\"] = final_df[\"sentence_length\"] - final_df[\"entity_2_index\"] - 1\n",
    "\n",
    "        ## Calculating root-entity distances for each sentence\n",
    "        final_df[\"entity_1_root_distance\"] = final_df.apply(lambda x: x['root_index'] - x['entity_1_index'], axis=1)\n",
    "        final_df[\"entity_2_root_distance\"] = final_df.apply(lambda x: x['entity_2_index'] - x['root_index'], axis=1)\n",
    "\n",
    "        # Expand tuples\n",
    "        final_df[['entity 1 name', 'entity 2 name']] = final_df['entity_pairs'].apply(pd.Series)\n",
    "\n",
    "        final_df[['entity_type_1', 'entity_type_2']] = final_df['ent_type_pairs'].apply(pd.Series)\n",
    "        final_df[['entity_pos_1', 'entity_pos_2']] = final_df['pos_pairs'].apply(pd.Series)\n",
    "        final_df[['entity_dep_1', 'entity_dep_2']] = final_df['dep_pairs'].apply(pd.Series)\n",
    "        final_df[['entity_tag_1', 'entity_tag_2']] = final_df['tag_pairs'].apply(pd.Series)\n",
    "        final_df[[\"shortest_distance\", \"shortest_path\"]] = final_df.apply(lambda x : generateNetwork(x[\"sentence\"], x[\"entity_1_index\"], x[\"entity_2_index\"]), axis = 1, result_type=\"expand\")\n",
    "\n",
    "        final_df[\"entity_type_1\"] = final_df[\"entity_type_1\"].apply(lambda x : x.split(\"-\")[0])\n",
    "        final_df[\"entity_type_2\"] = final_df[\"entity_type_2\"].apply(lambda x : x.split(\"-\")[0])\n",
    "        \n",
    "        final_df = final_df.drop([\"entity_pairs\", \"ent_type_pairs\", \"pos_pairs\", \"dep_pairs\", \"sentence\", \"tag_pairs\"], axis = 1)\n",
    "        return final_df\n",
    "    except :\n",
    "        print(\"Failed to execute file : \", file)\n",
    "        print(\"Error : \", sys.exc_info())\n",
    "        print(traceback.format_exc())\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ab79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sent_index', 'sentence_length', 'root_word', 'root_index',\n",
    "       'entity_1_index', 'entity_2_index', 'entity_distance',\n",
    "       'no_words_before_entity_1', 'no_words_after_entity_2',\n",
    "       'entity_1_root_distance', 'entity_2_root_distance', 'entity 1 name',\n",
    "       'entity 2 name', 'entity_type_1', 'entity_type_2', 'entity_pos_1',\n",
    "       'entity_pos_2', 'entity_dep_1', 'entity_dep_2', 'entity_tag_1',\n",
    "       'entity_tag_2', 'shortest_distance', 'label']\n",
    "\n",
    "def getFeaturesWithlabels(relation_file, tagged_tokens_file):\n",
    "    try:\n",
    "        final_df = getFeatures(tagged_tokens_file)\n",
    "        relations = pd.read_table(relation_file)\n",
    "        result = final_df.merge(relations, on=[\"entity 1 name\", \"entity 2 name\"], sort=False)\n",
    "        result.drop_duplicates(subset=[\"sent_index\", \"entity 1 name\", \"entity 2 name\"], keep='first', inplace=True, ignore_index=True)\n",
    "        if \"span\" not in result.keys():\n",
    "            display(result)\n",
    "        result[\"label\"] = result[\"span\"].apply(lambda x : 1 if isinstance(x, str) else 0)\n",
    "        return result\n",
    "    except:\n",
    "        print(\"Skipping run for : \", relation_file, tagged_tokens_file)\n",
    "        print(sys.exc_info())\n",
    "        display(final_df)\n",
    "#         display(relations)\n",
    "        print(traceback.format_exc())\n",
    "        return pd.DataFrame()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23490d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ACE2005\"\n",
    "ground_truth_path = f\"/Users/anishajauhari/Desktop/Sem 4/Independent Study /Dataset/relex/{dataset}/ground_truth\"\n",
    "tagged_token_path = f\"/Users/anishajauhari/Desktop/Sem 4/Independent Study /Dataset/relex/{dataset}/tagged_tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(tagged_token_path)\n",
    "features_with_labels = pd.DataFrame()\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".csv\"):\n",
    "        print(\"File : \", file)\n",
    "        tagged_tokens_file = f\"{tagged_token_path}/{file}\"\n",
    "        relation_file = f\"{ground_truth_path}/{file}\"\n",
    "        temp = getFeaturesWithlabels(relation_file, tagged_tokens_file)\n",
    "        features_with_labels = pd.concat([features_with_labels, temp])\n",
    "\n",
    "display(features_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_labels.to_csv(\"/Users/anishajauhari/Desktop/Sem 4/Independent Study /ResponsibleRelationExtraction/Features/n_features_\"+dataset+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54b8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
